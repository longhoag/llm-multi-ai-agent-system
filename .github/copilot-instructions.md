# Copilot Instructions for llm-multi-ai-agent-system

## Project Overview
This repository implements a multi-agent LLM pipeline for stock price prediction using AWS services. Agents are specialized for data ingestion, preprocessing, and model training, communicating via Python async messaging within a single app for simplicity.

## Architecture & Major Components
- **Agents**:
  - **Data Ingestion Agent**: Fetches financial data (Alpha Vantage API, others), stores in AWS S3. Uses `boto3`, AWS Glue, Kinesis.
  - **Preprocessing Agent**: Cleans, transforms, and feature-engineers data. Uses SageMaker Processing, AWS Glue, Pandas, Scikit-learn.
  - **Training Agent**: Selects algorithms, launches SageMaker training jobs (XGBoost, PyTorch, TensorFlow), tracks metrics.
- **Communication**: Agents interact via Python async messaging (e.g., `asyncio`, `queue` or similar patterns).
- **Persistence**: Data in S3; agent states in DynamoDB or RDS.
- **LLM Framework**: Uses `langchain` for agent orchestration and prompt management with **GPT-5-mini** as the primary model for complex reasoning tasks.

## Agent Coordination Model
The system uses a **hybrid coordination model** combining peer-to-peer and hierarchical patterns:

- **Peer-to-Peer Communication**: Agents communicate directly via async message bus without strict command hierarchy. Each agent operates independently with clear boundaries.
- **Hierarchical Orchestration**: System Orchestrator provides high-level workflow coordination, monitoring, and failure recovery. Pipeline follows logical hierarchy: Ingestion → Preprocessing → Training.
- **GPT-5-mini Powered Dynamic Coordination**: LLM makes real-time coordination decisions based on system load, agent status, and workflow complexity. Can switch between parallel and sequential processing modes.
- **Message-Driven Architecture**: All inter-agent communication uses structured messages with correlation IDs for workflow tracking.

## Developer Workflows
- **Dependency Management**: Use [Poetry](https://python-poetry.org/) for Python dependencies. Always update `pyproject.toml` and run `poetry install`.
- **Logging**: Use `loguru` for logging across all agents. Avoid `logging` module unless integrating with legacy code.
- **AWS Integration**: Use `boto3` for AWS SDK calls. For ETL, prefer AWS Glue scripts (can be generated by LLM agents).
- **Testing**: Place tests in a `tests/` directory. Use `pytest` as the default test runner.
- **Scripts & Notebooks**: Store exploratory notebooks in `notebooks/`. Scripts for agent orchestration go in `scripts/` or `src/`.

## Project-Specific Conventions
- **Agent Boundaries**: Each agent should be a separate Python module/class with clear responsibilities. Cross-agent calls should use async patterns.
- **External Data**: Always fetch data via the Data Ingestion Agent; do not bypass this for quick experiments.
- **Glue/Spark Scripts**: Generate via LLM when possible; store in `scripts/` or `glue_jobs/`.
- **Model Training**: Use SageMaker jobs for all production training. Local training is for prototyping only.
- **State Management**: Persist agent states in DynamoDB or RDS, not in local files.
- **LLM Model Selection**: Use GPT-5-mini for both complex reasoning and routine tasks (latest, most cost-effective, high-performance model for all agent operations).
- **Coordination Patterns**: Implement BaseAgent class for consistent message handling. Use AgentCoordinator for workflow orchestration. Always include correlation IDs in messages.

## Integration Points & Patterns
- **Alpha Vantage API**: Use official client or REST calls; wrap in ingestion agent.
- **AWS Services**: Prefer SDK calls over CLI. For ETL, use Glue jobs; for streaming, use Kinesis.
- **Langchain**: Use for prompt management and agent orchestration. Reference `src/agents/` for examples.
- **LLM Integration**: Configure OpenAI client with GPT-5-mini for all AI operations (latest model with optimal balance of capability and cost).
- **Message Bus**: Use asyncio queues for message routing. Implement publish/subscribe pattern for agent communication.

## Key Files & Directories
- `README.md`: High-level architecture and agent responsibilities.
- `pyproject.toml`: Poetry dependency management.
- `src/agents/`: Agent implementations with BaseAgent inheritance.
- `src/coordination/`: Agent coordination and workflow orchestration.
- `src/messaging/`: Message bus and routing implementation.
- `scripts/`, `glue_jobs/`: Orchestration and ETL scripts.
- `tests/`: Test suite with agent integration tests.
- `notebooks/`: Exploratory analysis.

## Example Patterns
- Agent class structure with GPT-5-mini integration:
  ```python
  from langchain.llms import OpenAI
  from src.agents.base_agent import BaseAgent
  
  class DataIngestionAgent(BaseAgent):
      def __init__(self, agent_id: str, message_bus):
          super().__init__(agent_id, message_bus)
          self.llm = OpenAI(model="gpt-5-mini", temperature=0)  # For strategic decisions
          self.routine_llm = OpenAI(model="gpt-5-mini", temperature=0)  # For routine tasks
      
      async def process_message(self, message):
          # Use GPT-5-mini for coordination strategy decisions
          strategy = await self.llm.agenerate([strategy_prompt])
          # Process and route message based on strategy
  ```
- Message structure:
  ```python
  from dataclasses import dataclass
  
  @dataclass
  class AgentMessage:
      sender: str
      recipient: str
      message_type: str
      payload: dict
      correlation_id: str = None
  ```
- Coordination pattern:
  ```python
  from src.coordination.agent_coordinator import AgentCoordinator
  
  coordinator = AgentCoordinator(message_bus, config)
  await coordinator.coordinate_workflow("workflow_123", ["AAPL", "GOOGL"])
  ```
- Logging:
  ```python
  from loguru import logger
  logger.info("Agent started with hybrid coordination model")
  ```
- AWS SDK usage:
  ```python
  import boto3
  s3 = boto3.client('s3')
  s3.upload_file(...)
  ```

---

**Review these instructions for accuracy and completeness. Suggest improvements if any section is unclear or missing project-specific details.**